---
title: 'Gemini æ¨¡å‹'
sidebarTitle: 'Gemini'
description: 'Google Geminiç³»åˆ—æ¨¡å‹çš„è¯¦ç»†ä½¿ç”¨æŒ‡å—'
---

## æ¨¡å‹ç®€ä»‹

Google Geminiæ˜¯è°·æ­Œå¼€å‘çš„å…ˆè¿›å¤šæ¨¡æ€AIæ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„æ–‡æœ¬ç†è§£ã€ä»£ç ç”Ÿæˆã€å›¾åƒåˆ†æå’Œæ¨ç†èƒ½åŠ›ã€‚åœ¨è€å¼ APIå¹³å°ä¸Šï¼Œä½ å¯ä»¥ä½¿ç”¨Geminiçš„æœ€æ–°æ¨¡å‹ç‰ˆæœ¬ã€‚

## ğŸš€ æ¨¡å‹æ¦‚è§ˆ

<CardGroup cols={2}>
  <Card
    title="Gemini 1.5 Pro"
    icon="google"
    href="#gemini-1-5-pro"
  >
    æœ€æ–°æ——èˆ°æ¨¡å‹ï¼Œæ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡å’Œå¤šæ¨¡æ€
  </Card>
  <Card
    title="Gemini 1.5 Flash"
    icon="zap"
    href="#gemini-1-5-flash"
  >
    è½»é‡å¿«é€Ÿç‰ˆæœ¬ï¼Œé«˜æ€§ä»·æ¯”é€‰æ‹©
  </Card>
  <Card
    title="Gemini 1.0 Pro"
    icon="star"
    href="#gemini-1-0-pro"
  >
    ç»å…¸ç‰ˆæœ¬ï¼Œç¨³å®šå¯é 
  </Card>
  <Card
    title="Gemini Vision"
    icon="eye"
    href="#gemini-vision"
  >
    ä¸“é—¨ä¼˜åŒ–çš„è§†è§‰ç†è§£æ¨¡å‹
  </Card>
</CardGroup>

## Gemini 1.5 Pro (æ¨è)

Gemini 1.5 Proæ˜¯Googleæœ€æ–°çš„æ——èˆ°æ¨¡å‹ï¼Œæ”¯æŒé•¿è¾¾200ä¸‡tokençš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

**æ¨¡å‹IDï¼š** `gemini-1.5-pro-latest`

**ç‰¹ç‚¹ï¼š**
- ğŸ§  å¼ºå¤§çš„æ¨ç†å’Œåˆ†æèƒ½åŠ›
- ğŸ“š è¶…é•¿ä¸Šä¸‹æ–‡æ”¯æŒï¼ˆ2M tokensï¼‰
- ğŸ–¼ï¸ å¤šæ¨¡æ€èƒ½åŠ›ï¼šæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘
- ğŸ” ä¼˜ç§€çš„æ–‡æ¡£ç†è§£èƒ½åŠ›
- ğŸ’» å“è¶Šçš„ä»£ç ç”Ÿæˆèƒ½åŠ›
- ğŸŒ ä¼˜ç§€çš„å¤šè¯­è¨€æ”¯æŒ

**ä»·æ ¼ï¼š** è¾“å…¥ $3.5/1M tokensï¼Œè¾“å‡º $10.5/1M tokens

<CodeGroup>

```python Python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_API_KEY",
    base_url="https://api.laozhang.ai/v1"
)

# åŸºç¡€å¯¹è¯
response = client.chat.completions.create(
    model="gemini-1.5-pro-latest",
    messages=[
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æå’Œè§£å†³å¤æ‚é—®é¢˜"
        },
        {
            "role": "user",
            "content": "è¯·åˆ†æä¸€ä¸‹äººå·¥æ™ºèƒ½åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨å‰æ™¯"
        }
    ],
    max_tokens=1000,
    temperature=0.7
)

print(response.choices[0].message.content)
```

```javascript Node.js
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: 'YOUR_API_KEY',
  baseURL: 'https://api.laozhang.ai/v1'
});

const completion = await openai.chat.completions.create({
  model: 'gemini-1.5-pro-latest',
  messages: [
    {
      role: 'system',
      content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æå’Œè§£å†³å¤æ‚é—®é¢˜'
    },
    {
      role: 'user',
      content: 'è¯·åˆ†æä¸€ä¸‹äººå·¥æ™ºèƒ½åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨å‰æ™¯'
    }
  ],
  max_tokens: 1000,
  temperature: 0.7
});

console.log(completion.choices[0].message.content);
```

```bash cURL
curl -X POST "https://api.laozhang.ai/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "gemini-1.5-pro-latest",
    "messages": [
      {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æå’Œè§£å†³å¤æ‚é—®é¢˜"
      },
      {
        "role": "user",
        "content": "è¯·åˆ†æä¸€ä¸‹äººå·¥æ™ºèƒ½åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨å‰æ™¯"
      }
    ],
    "max_tokens": 1000,
    "temperature": 0.7
  }'
```

</CodeGroup>

### ğŸ“š è¶…é•¿æ–‡æ¡£åˆ†æ

Gemini 1.5 Proçš„è¶…é•¿ä¸Šä¸‹æ–‡ç‰¹åˆ«é€‚åˆå¤„ç†å¤§å‹æ–‡æ¡£ï¼š

```python
# å¤„ç†è¶…é•¿æ–‡æ¡£
long_document = """
[è¿™é‡Œæ˜¯ä¸€ä¸ªéå¸¸é•¿çš„æ–‡æ¡£ï¼Œå¯èƒ½åŒ…å«æ•°åä¸‡å­—çš„å†…å®¹ï¼Œ
æ¯”å¦‚å®Œæ•´çš„å­¦æœ¯è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£ã€å°è¯´ç­‰...]
"""

response = client.chat.completions.create(
    model="gemini-1.5-pro-latest",
    messages=[
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œæ“…é•¿æå–å…³é”®ä¿¡æ¯å’Œæ·±å…¥åˆ†æ"
        },
        {
            "role": "user",
            "content": f"""
            è¯·å¯¹ä»¥ä¸‹æ–‡æ¡£è¿›è¡Œå…¨é¢åˆ†æï¼š
            
            {long_document}
            
            è¯·æä¾›ï¼š
            1. æ–‡æ¡£æ‘˜è¦ï¼ˆ500å­—ä»¥å†…ï¼‰
            2. å…³é”®è§‚ç‚¹å’Œè®ºè¯
            3. æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
            4. ç»“è®ºå’Œå»ºè®®
            5. æ½œåœ¨é—®é¢˜å’Œæ”¹è¿›æ–¹å‘
            """
        }
    ],
    max_tokens=3000,
    temperature=0.3
)
```

### ğŸ–¼ï¸ å¤šæ¨¡æ€èƒ½åŠ›

Gemini 1.5 Proæ”¯æŒå›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘åˆ†æï¼š

```python
# å›¾åƒåˆ†æ
response = client.chat.completions.create(
    model="gemini-1.5-pro-latest",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„è®¾è®¡å…ƒç´ ã€è‰²å½©æ­é…å’Œè§†è§‰æ•ˆæœ"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://example.com/image.jpg",
                        "detail": "high"
                    }
                }
            ]
        }
    ],
    max_tokens=800
)

print(response.choices[0].message.content)
```

### ğŸ’» ä»£ç ç”Ÿæˆå’Œå®¡æŸ¥

```python
# å¤æ‚ä»£ç ç”Ÿæˆä»»åŠ¡
code_request = """
è¯·åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„Pythonç±»ï¼Œå®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š

1. ä¸€ä¸ªåˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿçš„å®¢æˆ·ç«¯
2. æ”¯æŒRediså’ŒMemcachedåç«¯
3. å®ç°è‡ªåŠ¨æ•…éšœè½¬ç§»
4. åŒ…å«è¿æ¥æ± ç®¡ç†
5. æ”¯æŒå¼‚æ­¥æ“ä½œ
6. æä¾›å®Œæ•´çš„é”™è¯¯å¤„ç†
7. åŒ…å«è¯¦ç»†çš„æ–‡æ¡£å’Œç±»å‹æç¤º
8. æä¾›å®Œæ•´çš„å•å…ƒæµ‹è¯•

è¯·ç¡®ä¿ä»£ç ç¬¦åˆPythonæœ€ä½³å®è·µï¼ŒåŒ…å«è¯¦ç»†æ³¨é‡Šã€‚
"""

response = client.chat.completions.create(
    model="gemini-1.5-pro-latest",
    messages=[
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„Pythonå¼€å‘ä¸“å®¶ï¼Œæ“…é•¿è®¾è®¡é«˜è´¨é‡çš„ç³»ç»Ÿæ¶æ„"
        },
        {
            "role": "user",
            "content": code_request
        }
    ],
    max_tokens=4000,
    temperature=0.2
)
```

## Gemini 1.5 Flash (é«˜æ€§ä»·æ¯”)

Gemini 1.5 Flashæ˜¯è½»é‡çº§ç‰ˆæœ¬ï¼Œæä¾›å¿«é€Ÿå“åº”å’Œä¼˜å¼‚çš„æ€§ä»·æ¯”ã€‚

### âš¡ æ ¸å¿ƒç‰¹æ€§

**æ¨¡å‹IDï¼š** `gemini-1.5-flash-latest`

**ç‰¹ç‚¹ï¼š**
- ğŸš€ è¶…å¿«å“åº”é€Ÿåº¦
- ğŸ’° æé«˜æ€§ä»·æ¯”
- ğŸ¯ ä¿æŒæ ¸å¿ƒèƒ½åŠ›
- ğŸ“± ç§»åŠ¨ç«¯å‹å¥½
- ğŸ”„ é€‚åˆé«˜é¢‘è°ƒç”¨

**ä»·æ ¼ï¼š** è¾“å…¥ $0.075/1M tokensï¼Œè¾“å‡º $0.3/1M tokens

### ğŸ® ä½¿ç”¨åœºæ™¯

<AccordionGroup>
  <Accordion icon="chat" title="å®æ—¶å¯¹è¯">
    ```python
    # å®æ—¶èŠå¤©åœºæ™¯
    def real_time_chat(user_message, conversation_history):
        messages = conversation_history + [
            {"role": "user", "content": user_message}
        ]
        
        response = client.chat.completions.create(
            model="gemini-1.5-flash-latest",
            messages=messages,
            max_tokens=500,
            temperature=0.7,
            stream=True  # æµå¼å“åº”
        )
        
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    ```
  </Accordion>

  <Accordion icon="language" title="å¿«é€Ÿç¿»è¯‘">
    ```python
    # æ‰¹é‡ç¿»è¯‘ä»»åŠ¡
    def batch_translate(texts, target_lang="ä¸­æ–‡"):
        batch_text = "\n".join([f"{i+1}. {text}" for i, text in enumerate(texts)])
        
        response = client.chat.completions.create(
            model="gemini-1.5-flash-latest",
            messages=[
                {
                    "role": "system",
                    "content": f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘ä¸“å®¶ï¼Œè¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_lang}"
                },
                {
                    "role": "user",
                    "content": batch_text
                }
            ],
            max_tokens=2000,
            temperature=0.2
        )
        
        return response.choices[0].message.content
    ```
  </Accordion>

  <Accordion icon="code" title="ä»£ç è¾…åŠ©">
    ```python
    # ä»£ç è¡¥å…¨å’Œä¼˜åŒ–
    def code_assistant(code, task="ä¼˜åŒ–"):
        response = client.chat.completions.create(
            model="gemini-1.5-flash-latest",
            messages=[
                {
                    "role": "system",
                    "content": f"ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿{task}ä»£ç "
                },
                {
                    "role": "user",
                    "content": f"è¯·{task}ä»¥ä¸‹ä»£ç ï¼š\n\n{code}"
                }
            ],
            max_tokens=1000,
            temperature=0.3
        )
        
        return response.choices[0].message.content
    ```
  </Accordion>
</AccordionGroup>

### ğŸ“± ç§»åŠ¨åº”ç”¨é›†æˆ

```python
# ç§»åŠ¨ç«¯ä¼˜åŒ–é…ç½®
mobile_config = {
    "model": "gemini-1.5-flash-latest",
    "max_tokens": 300,  # æ§åˆ¶å“åº”é•¿åº¦
    "temperature": 0.7,
    "timeout": 10,      # å¿«é€Ÿè¶…æ—¶
    "stream": True      # æµå¼å“åº”æå‡ä½“éªŒ
}

def mobile_chat(user_input):
    """ç§»åŠ¨ç«¯èŠå¤©åŠŸèƒ½"""
    response = client.chat.completions.create(
        **mobile_config,
        messages=[
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ç§»åŠ¨åŠ©æ‰‹ï¼Œå›ç­”ç®€æ´æ˜äº†"
            },
            {
                "role": "user",
                "content": user_input
            }
        ]
    )
    
    return response.choices[0].message.content
```

## Gemini 1.0 Pro (ç»å…¸ç‰ˆ)

ç¨³å®šå¯é çš„ç»å…¸ç‰ˆæœ¬ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒã€‚

### ğŸ›ï¸ æ ¸å¿ƒç‰¹æ€§

**æ¨¡å‹IDï¼š** `gemini-1.0-pro`

**ç‰¹ç‚¹ï¼š**
- ğŸ¯ ç¨³å®šå¯é çš„æ€§èƒ½
- ğŸ’¼ é€‚åˆç”Ÿäº§ç¯å¢ƒ
- ğŸ“Š è‰¯å¥½çš„å•†ä¸šåº”ç”¨è¡¨ç°
- ğŸ”„ é•¿æœŸæ”¯æŒ

**ä»·æ ¼ï¼š** è¾“å…¥ $0.5/1M tokensï¼Œè¾“å‡º $1.5/1M tokens

### ğŸ’¼ å•†ä¸šåº”ç”¨

```python
# å•†ä¸šåˆ†æå’ŒæŠ¥å‘Š
business_analysis = """
è¯·åˆ†æä»¥ä¸‹å…¬å¸çš„å­£åº¦è´¢åŠ¡æ•°æ®ï¼š

è¥æ”¶ï¼š$125M (åŒæ¯”å¢é•¿15%)
åˆ©æ¶¦ï¼š$32M (åŒæ¯”å¢é•¿8%)
ç”¨æˆ·æ•°ï¼š2.3M (åŒæ¯”å¢é•¿25%)
å‘˜å·¥æ•°ï¼š450äºº (åŒæ¯”å¢é•¿12%)

ä¸»è¦ä¸šåŠ¡ï¼š
- SaaSäº§å“ (70%è¥æ”¶)
- å’¨è¯¢æœåŠ¡ (20%è¥æ”¶)
- åŸ¹è®­ä¸šåŠ¡ (10%è¥æ”¶)

ç«äº‰å¯¹æ‰‹ï¼š
- å…¬å¸Aï¼šå¸‚åœºä»½é¢35%
- å…¬å¸Bï¼šå¸‚åœºä»½é¢28%
- æˆ‘ä»¬ï¼šå¸‚åœºä»½é¢12%

è¯·æä¾›è¯¦ç»†çš„ä¸šåŠ¡åˆ†æå’Œå‘å±•å»ºè®®ã€‚
"""

response = client.chat.completions.create(
    model="gemini-1.0-pro",
    messages=[
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„å•†ä¸šåˆ†æå¸ˆï¼Œæ“…é•¿è´¢åŠ¡åˆ†æå’Œæˆ˜ç•¥è§„åˆ’"
        },
        {
            "role": "user",
            "content": business_analysis
        }
    ],
    max_tokens=2000,
    temperature=0.3
)
```

## Gemini Vision (è§†è§‰ä¸“å®¶)

ä¸“é—¨é’ˆå¯¹è§†è§‰ä»»åŠ¡ä¼˜åŒ–çš„æ¨¡å‹ã€‚

### ğŸ‘ï¸ æ ¸å¿ƒç‰¹æ€§

**æ¨¡å‹IDï¼š** `gemini-pro-vision`

**ç‰¹ç‚¹ï¼š**
- ğŸ–¼ï¸ ä¸“ä¸šçš„å›¾åƒç†è§£èƒ½åŠ›
- ğŸ¨ è®¾è®¡å…ƒç´ åˆ†æ
- ğŸ“Š å›¾è¡¨å’Œæ•°æ®å¯è§†åŒ–è§£è¯»
- ğŸ—ï¸ å»ºç­‘å’Œç©ºé—´åˆ†æ
- ğŸ­ è‰ºæœ¯ä½œå“é‰´èµ

**ä»·æ ¼ï¼š** è¾“å…¥ $0.25/1M tokensï¼Œè¾“å‡º $0.5/1M tokens

### ğŸ¨ å›¾åƒåˆ†æåº”ç”¨

<CodeGroup>

```python è®¾è®¡åˆ†æ
# è®¾è®¡å…ƒç´ åˆ†æ
def analyze_design(image_url):
    response = client.chat.completions.create(
        model="gemini-pro-vision",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": """
                        è¯·ä»ä¸“ä¸šè®¾è®¡å¸ˆçš„è§’åº¦åˆ†æè¿™å¼ å›¾ç‰‡ï¼š
                        
                        1. è‰²å½©æ­é…å’Œè‰²å½©å¿ƒç†å­¦
                        2. æ„å›¾å’Œè§†è§‰å¹³è¡¡
                        3. å­—ä½“å’Œç‰ˆå¼è®¾è®¡
                        4. å“ç‰Œè¯†åˆ«å’Œä¸€è‡´æ€§
                        5. ç”¨æˆ·ä½“éªŒå’Œå¯ç”¨æ€§
                        6. æ”¹è¿›å»ºè®®
                        """
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": image_url}
                    }
                ]
            }
        ],
        max_tokens=1000
    )
    
    return response.choices[0].message.content
```

```python æ•°æ®å›¾è¡¨è§£è¯»
# å›¾è¡¨æ•°æ®åˆ†æ
def analyze_chart(chart_image_url):
    response = client.chat.completions.create(
        model="gemini-pro-vision",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": """
                        è¯·åˆ†æè¿™ä¸ªå›¾è¡¨ï¼š
                        
                        1. å›¾è¡¨ç±»å‹å’Œæ•°æ®ç»“æ„
                        2. å…³é”®æ•°æ®ç‚¹å’Œè¶‹åŠ¿
                        3. æ•°æ®çš„å«ä¹‰å’Œæ´å¯Ÿ
                        4. å¯èƒ½çš„é—®é¢˜å’Œå¼‚å¸¸
                        5. å»ºè®®çš„åç»­è¡ŒåŠ¨
                        """
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": chart_image_url}
                    }
                ]
            }
        ],
        max_tokens=800
    )
    
    return response.choices[0].message.content
```

</CodeGroup>

### ğŸ—ï¸ å»ºç­‘å’Œç©ºé—´åˆ†æ

```python
# å»ºç­‘è®¾è®¡åˆ†æ
def analyze_architecture(image_url):
    response = client.chat.completions.create(
        model="gemini-pro-vision",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": """
                        è¯·ä»å»ºç­‘å¸ˆçš„è§’åº¦åˆ†æè¿™å¼ å»ºç­‘å›¾ç‰‡ï¼š
                        
                        1. å»ºç­‘é£æ ¼å’Œè®¾è®¡ç†å¿µ
                        2. ç©ºé—´å¸ƒå±€å’ŒåŠŸèƒ½åˆ†åŒº
                        3. ææ–™é€‰æ‹©å’Œæ„é€ æŠ€æœ¯
                        4. ç¯å¢ƒé€‚åº”æ€§å’Œå¯æŒç»­æ€§
                        5. ç¾å­¦ä»·å€¼å’Œæ–‡åŒ–æ„ä¹‰
                        6. æ”¹è¿›å’Œä¼˜åŒ–å»ºè®®
                        """
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": image_url}
                    }
                ]
            }
        ],
        max_tokens=1200
    )
    
    return response.choices[0].message.content
```

## Geminiçš„ç‹¬ç‰¹ä¼˜åŠ¿

### 1. ğŸ§  å¼ºå¤§çš„æ¨ç†èƒ½åŠ›

Geminiåœ¨é€»è¾‘æ¨ç†å’Œé—®é¢˜è§£å†³æ–¹é¢è¡¨ç°å‡ºè‰²ï¼š

```python
# å¤æ‚é€»è¾‘æ¨ç†
logical_problem = """
æœ‰ä¸€ä¸ªå²›å±¿ï¼Œä½ç€ä¸¤ç§äººï¼š
1. è¯šå®çš„äººï¼Œæ€»æ˜¯è¯´çœŸè¯
2. æ’’è°çš„äººï¼Œæ€»æ˜¯è¯´å‡è¯

ä½ é‡åˆ°äº†ä¸‰ä¸ªäººAã€Bã€Cï¼Œä»–ä»¬è¯´ï¼š
- Aè¯´ï¼š"æˆ‘ä»¬ä¸‰ä¸ªäººä¸­è‡³å°‘æœ‰ä¸€ä¸ªæ˜¯è¯šå®çš„"
- Bè¯´ï¼š"Aæ˜¯æ’’è°çš„äºº"
- Cè¯´ï¼š"Bå’Œæˆ‘éƒ½æ˜¯æ’’è°çš„äºº"

è¯·é—®Aã€Bã€Cåˆ†åˆ«æ˜¯ä»€ä¹ˆäººï¼Ÿè¯·è¯¦ç»†è¯´æ˜æ¨ç†è¿‡ç¨‹ã€‚
"""

response = client.chat.completions.create(
    model="gemini-1.5-pro-latest",
    messages=[
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªé€»è¾‘æ¨ç†ä¸“å®¶ï¼Œæ“…é•¿è§£å†³å¤æ‚çš„é€»è¾‘é—®é¢˜"
        },
        {
            "role": "user",
            "content": logical_problem
        }
    ],
    max_tokens=1000,
    temperature=0.2
)
```

### 2. ğŸ“š è¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†

Gemini 1.5 Proæ”¯æŒé«˜è¾¾200ä¸‡tokençš„ä¸Šä¸‹æ–‡ï¼š

```python
# å¤„ç†è¶…é•¿å†…å®¹
def process_long_content(content_list):
    """å¤„ç†å¤šä¸ªé•¿æ–‡æ¡£"""
    combined_content = "\n\n".join([
        f"æ–‡æ¡£{i+1}:\n{content}" 
        for i, content in enumerate(content_list)
    ])
    
    response = client.chat.completions.create(
        model="gemini-1.5-pro-latest",
        messages=[
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æä¸“å®¶ï¼Œæ“…é•¿å¤„ç†å¤§é‡ä¿¡æ¯"
            },
            {
                "role": "user",
                "content": f"""
                è¯·åˆ†æä»¥ä¸‹å¤šä¸ªæ–‡æ¡£ï¼Œå¹¶æä¾›ï¼š
                1. æ–‡æ¡£é—´çš„å…³è”æ€§åˆ†æ
                2. å…±åŒä¸»é¢˜å’Œå·®å¼‚ç‚¹
                3. ç»¼åˆç»“è®ºå’Œå»ºè®®
                
                {combined_content}
                """
            }
        ],
        max_tokens=3000,
        temperature=0.3
    )
    
    return response.choices[0].message.content
```

### 3. ğŸ¯ ç²¾å‡†çš„å¤šæ¨¡æ€ç†è§£

Geminiåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼š

```python
# å¤šæ¨¡æ€åˆ†æ
def multimodal_analysis(image_urls, text_context):
    content = [
        {
            "type": "text",
            "text": f"èƒŒæ™¯ä¿¡æ¯ï¼š{text_context}\n\nè¯·ç»“åˆå›¾ç‰‡åˆ†æä»¥ä¸‹å†…å®¹ï¼š"
        }
    ]
    
    # æ·»åŠ å¤šå¼ å›¾ç‰‡
    for i, url in enumerate(image_urls):
        content.append({
            "type": "image_url",
            "image_url": {"url": url}
        })
    
    response = client.chat.completions.create(
        model="gemini-1.5-pro-latest",
        messages=[
            {
                "role": "user",
                "content": content
            }
        ],
        max_tokens=1500
    )
    
    return response.choices[0].message.content
```

## ä½¿ç”¨æŠ€å·§å’Œæœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥

<AccordionGroup>
  <Accordion icon="brain" title="å¤æ‚åˆ†æä»»åŠ¡">
    **æ¨èï¼š** Gemini 1.5 Pro
    
    - å­¦æœ¯ç ”ç©¶åˆ†æ
    - é•¿æ–‡æ¡£å¤„ç†
    - å¤šæ¨¡æ€ä»»åŠ¡
    - å¤æ‚æ¨ç†é—®é¢˜
    
    ```python
    # å¤æ‚ä»»åŠ¡é…ç½®
    complex_config = {
        "model": "gemini-1.5-pro-latest",
        "temperature": 0.3,
        "max_tokens": 3000
    }
    ```
  </Accordion>

  <Accordion icon="zap" title="å¿«é€Ÿå“åº”åœºæ™¯">
    **æ¨èï¼š** Gemini 1.5 Flash
    
    - å®æ—¶å¯¹è¯
    - å¿«é€Ÿç¿»è¯‘
    - ä»£ç è¡¥å…¨
    - ç§»åŠ¨åº”ç”¨
    
    ```python
    # å¿«é€Ÿå“åº”é…ç½®
    fast_config = {
        "model": "gemini-1.5-flash-latest",
        "temperature": 0.7,
        "max_tokens": 500
    }
    ```
  </Accordion>

  <Accordion icon="image" title="è§†è§‰ä»»åŠ¡">
    **æ¨èï¼š** Gemini Pro Vision
    
    - å›¾åƒåˆ†æ
    - è®¾è®¡å®¡æŸ¥
    - å›¾è¡¨è§£è¯»
    - è‰ºæœ¯é‰´èµ
    
    ```python
    # è§†è§‰ä»»åŠ¡é…ç½®
    vision_config = {
        "model": "gemini-pro-vision",
        "temperature": 0.4,
        "max_tokens": 1000
    }
    ```
  </Accordion>

  <Accordion icon="dollar-sign" title="æˆæœ¬æ•æ„Ÿåœºæ™¯">
    **æ¨èï¼š** Gemini 1.0 Pro
    
    - ç”Ÿäº§ç¯å¢ƒ
    - æ‰¹é‡å¤„ç†
    - åŸºç¡€å¯¹è¯
    - å†…å®¹ç”Ÿæˆ
    
    ```python
    # æˆæœ¬ä¼˜åŒ–é…ç½®
    cost_config = {
        "model": "gemini-1.0-pro",
        "temperature": 0.5,
        "max_tokens": 1000
    }
    ```
  </Accordion>
</AccordionGroup>

### 2. æ€§èƒ½ä¼˜åŒ–

<CardGroup cols={2}>
  <Card
    title="ä¸Šä¸‹æ–‡ç®¡ç†"
    icon="memory"
  >
    å……åˆ†åˆ©ç”¨Geminiçš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›
    
    ```python
    def manage_context(conversation_history, max_context=1000000):
        # æ™ºèƒ½æˆªæ–­ä¸Šä¸‹æ–‡
        total_tokens = sum(len(msg["content"]) for msg in conversation_history)
        
        if total_tokens > max_context:
            # ä¿ç•™æœ€é‡è¦çš„æ¶ˆæ¯
            important_messages = conversation_history[-20:]  # æœ€è¿‘20æ¡
            system_message = conversation_history[0]  # ç³»ç»Ÿæ¶ˆæ¯
            conversation_history = [system_message] + important_messages
        
        return conversation_history
    ```
  </Card>

  <Card
    title="å¹¶è¡Œå¤„ç†"
    icon="parallel"
  >
    å¹¶è¡Œå¤„ç†å¤šä¸ªä»»åŠ¡æé«˜æ•ˆç‡
    
    ```python
    import asyncio
    
    async def parallel_analysis(tasks):
        async def process_task(task):
            response = await client.chat.completions.create(
                model="gemini-1.5-flash-latest",
                messages=[{"role": "user", "content": task}],
                max_tokens=500
            )
            return response.choices[0].message.content
        
        results = await asyncio.gather(*[process_task(task) for task in tasks])
        return results
    ```
  </Card>

  <Card
    title="ç¼“å­˜ç­–ç•¥"
    icon="database"
  >
    ç¼“å­˜å¸¸ç”¨ç»“æœå‡å°‘APIè°ƒç”¨
    
    ```python
    from functools import lru_cache
    import hashlib
    
    @lru_cache(maxsize=1000)
    def cached_gemini_call(prompt_hash, model):
        # å®é™…çš„APIè°ƒç”¨
        return call_gemini_api(prompt_hash, model)
    
    def get_cached_response(prompt, model):
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        return cached_gemini_call(prompt_hash, model)
    ```
  </Card>

  <Card
    title="æµå¼å“åº”"
    icon="stream"
  >
    ä½¿ç”¨æµå¼å“åº”æå‡ç”¨æˆ·ä½“éªŒ
    
    ```python
    def stream_response(prompt, model="gemini-1.5-flash-latest"):
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            stream=True,
            max_tokens=1000
        )
        
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    ```
  </Card>
</CardGroup>

### 3. é”™è¯¯å¤„ç†å’Œç›‘æ§

```python
import time
import logging
from typing import Optional

class GeminiClient:
    def __init__(self, api_key: str, max_retries: int = 3):
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.laozhang.ai/v1"
        )
        self.max_retries = max_retries
        self.logger = logging.getLogger(__name__)
    
    def robust_completion(self, **kwargs) -> Optional[str]:
        """å¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†çš„å®Œæˆè¯·æ±‚"""
        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(**kwargs)
                
                # è®°å½•ä½¿ç”¨æƒ…å†µ
                self.log_usage(kwargs.get('model'), response.usage)
                
                return response.choices[0].message.content
                
            except Exception as e:
                self.logger.error(f"Attempt {attempt + 1} failed: {e}")
                
                if attempt == self.max_retries - 1:
                    raise e
                
                # æŒ‡æ•°é€€é¿
                time.sleep(2 ** attempt)
        
        return None
    
    def log_usage(self, model: str, usage):
        """è®°å½•ä½¿ç”¨æƒ…å†µ"""
        self.logger.info(f"Model: {model}, "
                        f"Input tokens: {usage.prompt_tokens}, "
                        f"Output tokens: {usage.completion_tokens}, "
                        f"Total: {usage.total_tokens}")

# ä½¿ç”¨ç¤ºä¾‹
gemini_client = GeminiClient("YOUR_API_KEY")
result = gemini_client.robust_completion(
    model="gemini-1.5-pro-latest",
    messages=[{"role": "user", "content": "Hello, Gemini!"}]
)
```

## ä¸å…¶ä»–æ¨¡å‹çš„å¯¹æ¯”

### Gemini vs GPT-4

<AccordionGroup>
  <Accordion icon="file-text" title="é•¿æ–‡æœ¬å¤„ç†">
    **Geminiä¼˜åŠ¿ï¼š**
    - æ”¯æŒé«˜è¾¾200ä¸‡tokençš„ä¸Šä¸‹æ–‡
    - æ›´å¥½çš„é•¿æ–‡æ¡£ç†è§£èƒ½åŠ›
    - ä¼˜ç§€çš„ä¿¡æ¯æå–å’Œæ€»ç»“
    
    **é€‚ç”¨åœºæ™¯ï¼š** é•¿æ–‡æ¡£åˆ†æã€å¤§æ•°æ®å¤„ç†ã€å¤æ‚ç ”ç©¶
  </Accordion>

  <Accordion icon="eye" title="å¤šæ¨¡æ€èƒ½åŠ›">
    **Geminiä¼˜åŠ¿ï¼š**
    - åŸç”Ÿå¤šæ¨¡æ€è®¾è®¡
    - æ›´å¼ºçš„å›¾åƒç†è§£èƒ½åŠ›
    - æ”¯æŒè§†é¢‘å’ŒéŸ³é¢‘åˆ†æ
    
    **é€‚ç”¨åœºæ™¯ï¼š** è§†è§‰åˆ†æã€åª’ä½“å¤„ç†ã€è®¾è®¡å®¡æŸ¥
  </Accordion>

  <Accordion icon="dollar-sign" title="æˆæœ¬æ•ˆç›Š">
    **Geminiä¼˜åŠ¿ï¼š**
    - æ›´å…·ç«äº‰åŠ›çš„ä»·æ ¼
    - é«˜æ€§ä»·æ¯”çš„Flashç‰ˆæœ¬
    - é•¿ä¸Šä¸‹æ–‡çš„æ€§ä»·æ¯”ä¼˜åŠ¿
    
    **é€‚ç”¨åœºæ™¯ï¼š** æˆæœ¬æ•æ„Ÿçš„åº”ç”¨ã€å¤§è§„æ¨¡éƒ¨ç½²
  </Accordion>
</AccordionGroup>

### Gemini vs Claude

<AccordionGroup>
  <Accordion icon="brain" title="æ¨ç†èƒ½åŠ›">
    **Geminiä¼˜åŠ¿ï¼š**
    - å¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›
    - ä¼˜ç§€çš„é€»è¾‘åˆ†æèƒ½åŠ›
    - æ›´å¥½çš„ä»£ç ç”Ÿæˆèƒ½åŠ›
    
    **é€‚ç”¨åœºæ™¯ï¼š** æ•°å­¦é—®é¢˜ã€é€»è¾‘æ¨ç†ã€ç¼–ç¨‹ä»»åŠ¡
  </Accordion>

  <Accordion icon="globe" title="å¤šè¯­è¨€æ”¯æŒ">
    **Geminiä¼˜åŠ¿ï¼š**
    - æ›´å¹¿æ³›çš„è¯­è¨€æ”¯æŒ
    - æ›´å¥½çš„è·¨è¯­è¨€ç†è§£èƒ½åŠ›
    - ä¼˜ç§€çš„æœ¬åœ°åŒ–èƒ½åŠ›
    
    **é€‚ç”¨åœºæ™¯ï¼š** å›½é™…åŒ–åº”ç”¨ã€å¤šè¯­è¨€å†…å®¹å¤„ç†
  </Accordion>
</AccordionGroup>

---

<CardGroup cols={2}>
  <Card
    title="æ¢ç´¢å›½äº§å¤§æ¨¡å‹"
    icon="flag"
    href="/api-reference/chinese-models"
  >
    äº†è§£é€šä¹‰åƒé—®ã€æ–‡å¿ƒä¸€è¨€ã€æ™ºè°±ChatGLMç­‰å›½äº§å¤§æ¨¡å‹
  </Card>
  <Card
    title="æŸ¥çœ‹å®Œæ•´APIå‚è€ƒ"
    icon="code"
    href="/api-reference/chat-completions"
  >
    æŸ¥çœ‹è¯¦ç»†çš„APIè°ƒç”¨å‚æ•°å’Œç¤ºä¾‹
  </Card>
</CardGroup> 