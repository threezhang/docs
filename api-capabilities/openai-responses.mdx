---
title: OpenAI Responses API æ”¯æŒ
description: æ–°ä¸€ä»£æ™ºèƒ½ä½“API - ç»“åˆå¯¹è¯è¡¥å…¨çš„ç®€æ´æ€§ä¸å¼ºå¤§çš„å·¥å…·è°ƒç”¨èƒ½åŠ›
icon: "message-circle-reply"
---

# /v1/responses è¯·æ±‚ç«¯ç‚¹ ä»‹ç»

APIæ˜“ å…¨é¢æ”¯æŒ OpenAI æœ€æ–°çš„ Responses APIï¼Œè¿™æ˜¯ 2025å¹´3æœˆæ¨å‡ºçš„æ–°ä¸€ä»£æ™ºèƒ½ä½“æ„å»ºæ¥å£ã€‚Responses API ç»“åˆäº† Chat Completions çš„ç®€æ´æ€§ä¸ Assistants API çš„å·¥å…·ä½¿ç”¨å’ŒçŠ¶æ€ç®¡ç†èƒ½åŠ›ï¼Œä¸ºå¼€å‘è€…æä¾›æ›´çµæ´»ã€æ›´å¼ºå¤§çš„AIåº”ç”¨æ„å»ºä½“éªŒã€‚

<Note>
  **æ–°ä¸€ä»£API**ï¼šResponses API æ˜¯ Chat Completions çš„è¶…é›†ï¼Œæä¾› Chat Completions åŠŸèƒ½çš„åŒæ—¶ï¼Œè¿˜æ”¯æŒå†…ç½®å·¥å…·ã€çŠ¶æ€ç®¡ç†ç­‰é«˜çº§ç‰¹æ€§ã€‚â€”â€” ä½†æ˜¯å®ƒä»…æ”¯æŒå°‘æ•°æ–°çš„ OpenAI æ¨¡å‹ï¼Œå…·ä½“è¯·çœ‹ä¸‹æ–‡ã€‚
</Note>

## ğŸš€ æ ¸å¿ƒç‰¹æ€§

<CardGroup cols={2}>
  <Card
    title="å†…ç½®å·¥å…·æ”¯æŒ"
    icon="codesandbox"
  >
    Webæœç´¢ã€æ–‡ä»¶æœç´¢ã€ä»£ç è§£é‡Šå™¨ã€å‡½æ•°è°ƒç”¨ç­‰ä¸°å¯Œå·¥å…·
  </Card>
  <Card
    title="çŠ¶æ€ç®¡ç†"
    icon="database"
  >
    é€šè¿‡ previous_response_id ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡å’ŒçŠ¶æ€
  </Card>
  <Card
    title="æ¨ç†ä¿æŒ"
    icon="brain"
  >
    O3/O4-mini æ¨ç†æ¨¡å‹çš„æ¨ç†ä»¤ç‰Œåœ¨è¯·æ±‚é—´ä¿æŒè¿ç»­
  </Card>
  <Card
    title="å®Œå…¨å…¼å®¹"
    icon="scale"
  >
    æ”¯æŒæ‰€æœ‰æ”¯æŒå·¥å…·çš„ GPT-4.1ã€O3ç³»åˆ—æ¨¡å‹
  </Card>
</CardGroup>

## ğŸ“‹ æ”¯æŒçš„æ¨¡å‹

### æ¨ç†æ¨¡å‹ï¼ˆæ¨èï¼‰
- **O3 ç³»åˆ—**ï¼š`o3`, `o3-pro`, `o4-mini`
- **ç‰¹è‰²**ï¼šæ¨ç†ä»¤ç‰Œè·¨è¯·æ±‚ä¿æŒï¼Œæä¾›æ›´æ™ºèƒ½çš„ä¸Šä¸‹æ–‡ç†è§£

### å¯¹è¯æ¨¡å‹
- **GPT-4.1 ç³»åˆ—**ï¼š`gpt-4.1`, `gpt-4.1-mini`
- **ç‰¹è‰²**ï¼šå¼ºå¤§çš„å·¥å…·è°ƒç”¨å’Œå¤šæ¨¡æ€èƒ½åŠ›

<Warning>
  **æ¨¡å‹è¦æ±‚**ï¼šåªæœ‰è¾ƒæ–°çš„æ¨¡å‹æ‰æ”¯æŒ `/v1/responses` ç«¯ç‚¹ã€‚æ—§æ¨¡å‹å¦‚ GPT-3.5 ä¸æ”¯æŒæ­¤æ¥å£ã€‚
</Warning>

## ğŸ”§ åŸºç¡€ç”¨æ³•

### ç®€å•å¯¹è¯

<CodeGroup>

```bash cURL
curl https://api.apiyi.com/v1/responses \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "gpt-4.1",
    "input": "Hello! How can you help me today?",
    "instructions": "You are a helpful assistant."
  }'
```

```python Python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_API_KEY",
    base_url="https://api.apiyi.com/v1"
)

response = client.responses.create(
    model="gpt-4.1",
    input="Hello! How can you help me today?",
    instructions="You are a helpful assistant."
)

print(response.output[0].content[0].text)
```

```javascript Node.js
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: 'YOUR_API_KEY',
  baseURL: 'https://api.apiyi.com/v1'
});

const response = await openai.responses.create({
  model: 'gpt-4.1',
  input: 'Hello! How can you help me today?',
  instructions: 'You are a helpful assistant.'
});

console.log(response.output[0].content[0].text);
```

</CodeGroup>

### å®é™…å“åº”ç¤ºä¾‹

åŸºäºæ‚¨çš„æµ‹è¯•ç»“æœï¼Œä»¥ä¸‹æ˜¯å®Œæ•´çš„å“åº”æ ¼å¼ï¼š

```json
{
  "id": "resp_6884fcab4930819dbbc02f15cbe63f6c0a92c38ff214d10a",
  "object": "response",
  "created_at": 1753545899,
  "status": "completed",
  "background": false,
  "error": null,
  "incomplete_details": null,
  "instructions": "You are a helpful assistant.",
  "max_output_tokens": null,
  "max_tool_calls": null,
  "model": "gpt-4.1-2025-04-14",
  "output": [
    {
      "id": "msg_6884fcab8f18819dbcdf349f01b424f80a92c38ff214d10a",
      "type": "message",
      "status": "completed",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Hello! How can I assist you today?"
        }
      ],
      "role": "assistant"
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "prompt_cache_key": null,
  "reasoning": {
    "effort": null,
    "summary": null
  },
  "safety_identifier": null,
  "service_tier": "default",
  "store": true,
  "temperature": 1.0,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_logprobs": 0,
  "top_p": 1.0,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 19,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 10,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 29
  },
  "user": null,
  "metadata": {}
}
```

## ğŸ“Š è¯·æ±‚å‚æ•°è¯¦è§£

### å¿…éœ€å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `model` | string | æ¨¡å‹åç§°ï¼Œå¦‚ `gpt-4.1`, `o3` |
| `input` | string | ç”¨æˆ·è¾“å…¥å†…å®¹ |

### å¯é€‰å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `instructions` | string | null | ç³»ç»ŸæŒ‡ä»¤ï¼Œå®šä¹‰åŠ©æ‰‹è¡Œä¸º |
| `previous_response_id` | string | null | ä¸Šä¸€ä¸ªå“åº”çš„ IDï¼Œç”¨äºç»´æŠ¤ä¸Šä¸‹æ–‡ |
| `temperature` | float | 1.0 | æ§åˆ¶è¾“å‡ºéšæœºæ€§ (0-2) |
| `max_output_tokens` | int | null | æœ€å¤§è¾“å‡ºä»¤ç‰Œæ•° |
| `tools` | array | [] | å¯ç”¨å·¥å…·åˆ—è¡¨ |
| `tool_choice` | string | "auto" | å·¥å…·é€‰æ‹©ç­–ç•¥ |
| `parallel_tool_calls` | boolean | true | æ˜¯å¦å…è®¸å¹¶è¡Œå·¥å…·è°ƒç”¨ |
| `store` | boolean | true | æ˜¯å¦å­˜å‚¨å¯¹è¯ç”¨äºè®­ç»ƒ |
| `metadata` | object | {} | è‡ªå®šä¹‰å…ƒæ•°æ® |

## ğŸ› ï¸ å†…ç½®å·¥å…·æ”¯æŒ

### 1. å‡½æ•°è°ƒç”¨

```python
response = client.responses.create(
    model="gpt-4.1",
    input="What's the weather like in Beijing?",
    instructions="You are a helpful weather assistant.",
    tools=[
        {
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get current weather for a city",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "City name"
                        }
                    },
                    "required": ["city"]
                }
            }
        }
    ]
)
```

### 2. ä»£ç è§£é‡Šå™¨

```python
response = client.responses.create(
    model="gpt-4.1", 
    input="Create a chart showing sales data: Jan:100, Feb:150, Mar:120",
    instructions="You are a data analyst. Use code interpreter to create visualizations.",
    tools=[{"type": "code_interpreter"}]
)
```

### 3. æ–‡ä»¶æœç´¢

```python
response = client.responses.create(
    model="gpt-4.1",
    input="Search for information about quarterly reports",
    instructions="You are a document analyst.",
    tools=[{"type": "file_search"}]
)
```

## ğŸ”„ çŠ¶æ€ç®¡ç†

### ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡

```python
# ç¬¬ä¸€è½®å¯¹è¯
response1 = client.responses.create(
    model="gpt-4.1",
    input="My name is Alice. Please remember this.",
    instructions="You are a helpful assistant with good memory."
)

# ç¬¬äºŒè½®å¯¹è¯ - ä½¿ç”¨ previous_response_id ç»´æŠ¤ä¸Šä¸‹æ–‡
response2 = client.responses.create(
    model="gpt-4.1", 
    input="What's my name?",
    instructions="You are a helpful assistant with good memory.",
    previous_response_id=response1.id
)

print(response2.output[0].content[0].text)  # åº”è¯¥å›ç­” "Alice"
```

### å¤šè½®å·¥å…·è°ƒç”¨

```python
def multi_turn_conversation():
    response_id = None
    
    for user_input in ["What's 2+2?", "Now multiply that by 3", "And divide by 2"]:
        response = client.responses.create(
            model="o3",
            input=user_input,
            instructions="You are a math tutor. Show your reasoning.",
            previous_response_id=response_id,
            tools=[{"type": "code_interpreter"}]
        )
        
        print(f"User: {user_input}")
        print(f"Assistant: {response.output[0].content[0].text}")
        
        response_id = response.id  # ä¿æŒä¸Šä¸‹æ–‡
```

## ğŸ“ˆ æ¨ç†æ¨¡å‹ç‰¹æ€§

### O3/O4-mini æ¨ç†ä¿æŒ

æ¨ç†æ¨¡å‹åœ¨ Responses API ä¸­å…·æœ‰ç‰¹æ®Šä¼˜åŠ¿ï¼š

```python
# ä½¿ç”¨ O3 è¿›è¡Œå¤æ‚æ¨ç†
response = client.responses.create(
    model="o3",
    input="Solve this step by step: If a train travels 120km in 2 hours, then speeds up 20% for the next hour, how far did it travel in total?",
    instructions="Think through this problem step by step, showing all reasoning."
)

# æŸ¥çœ‹æ¨ç†è¿‡ç¨‹
reasoning_tokens = response.usage.output_tokens_details.reasoning_tokens
print(f"Reasoning tokens used: {reasoning_tokens}")

# ç»§ç»­å¯¹è¯ï¼Œæ¨ç†ä¸Šä¸‹æ–‡ä¼šä¿æŒ
follow_up = client.responses.create(
    model="o3",
    input="Now what if the train slowed down 10% in the fourth hour?",
    previous_response_id=response.id
)
```

## ğŸ†š ä¸ Chat Completions å¯¹æ¯”

| ç‰¹æ€§ | Chat Completions | Responses API |
|------|-----------------|---------------|
| **åŸºç¡€å¯¹è¯** | âœ… æ”¯æŒ | âœ… æ”¯æŒ |
| **æµå¼å“åº”** | âœ… æ”¯æŒ | âœ… æ”¯æŒ |
| **å‡½æ•°è°ƒç”¨** | âœ… æ”¯æŒ | âœ… å¢å¼ºæ”¯æŒ |
| **å†…ç½®å·¥å…·** | âŒ ä¸æ”¯æŒ | âœ… ä¸°å¯Œå·¥å…· |
| **çŠ¶æ€ç®¡ç†** | âŒ æ— çŠ¶æ€ | âœ… æœ‰çŠ¶æ€ |
| **æ¨ç†ä¿æŒ** | âŒ ä¸æ”¯æŒ | âœ… O3/O4æ”¯æŒ |
| **æ–‡ä»¶æœç´¢** | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒ |
| **ä»£ç è§£é‡Šå™¨** | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒ |

### è¿ç§»ç¤ºä¾‹

ä» Chat Completions è¿ç§»åˆ° Responses APIï¼š

<CodeGroup>

```python Chat Completions
# æ—§æ–¹å¼
response = client.chat.completions.create(
    model="gpt-4.1",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ]
)
content = response.choices[0].message.content
```

```python Responses API
# æ–°æ–¹å¼
response = client.responses.create(
    model="gpt-4.1",
    input="Hello!",
    instructions="You are a helpful assistant."
)
content = response.output[0].content[0].text
```

</CodeGroup>

## ğŸ”§ é«˜çº§åŠŸèƒ½

### å¹¶è¡Œå·¥å…·è°ƒç”¨

```python
response = client.responses.create(
    model="gpt-4.1",
    input="Get weather for Beijing and Shanghai, then calculate travel time between them",
    instructions="You are a travel assistant.",
    parallel_tool_calls=True,
    tools=[
        {"type": "function", "function": {"name": "get_weather", ...}},
        {"type": "function", "function": {"name": "calculate_distance", ...}}
    ]
)
```

### è¾“å‡ºæ ¼å¼æ§åˆ¶

```python
response = client.responses.create(
    model="gpt-4.1",
    input="Summarize this data in JSON format",
    instructions="Always respond in valid JSON.",
    text={
        "format": {
            "type": "json_object"
        }
    }
)
```

### æ¨ç†åŠªåŠ›æ§åˆ¶ï¼ˆO3ç³»åˆ—ï¼‰

```python
response = client.responses.create(
    model="o3",
    input="Solve this complex physics problem",
    instructions="Think carefully and show detailed reasoning.",
    reasoning={
        "effort": "high"  # low, medium, high
    }
)
```

## ğŸ“Š å“åº”å­—æ®µè¯¦è§£

### æ ¸å¿ƒå­—æ®µ

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `id` | string | å“åº”å”¯ä¸€æ ‡è¯†ç¬¦ |
| `object` | string | å›ºå®šä¸º "response" |
| `created_at` | integer | åˆ›å»ºæ—¶é—´æˆ³ |
| `status` | string | çŠ¶æ€ï¼šcompleted/failed/in_progress |
| `model` | string | å®é™…ä½¿ç”¨çš„æ¨¡å‹ç‰ˆæœ¬ |
| `output` | array | è¾“å‡ºæ¶ˆæ¯æ•°ç»„ |
| `usage` | object | Token ä½¿ç”¨ç»Ÿè®¡ |

### è¾“å‡ºæ¶ˆæ¯æ ¼å¼

```json
{
  "id": "msg_xxx",
  "type": "message",
  "status": "completed",
  "content": [
    {
      "type": "output_text",
      "text": "å“åº”å†…å®¹",
      "annotations": [],
      "logprobs": []
    }
  ],
  "role": "assistant"
}
```

### ä½¿ç”¨ç»Ÿè®¡

```json
{
  "usage": {
    "input_tokens": 19,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 10,
    "output_tokens_details": {
      "reasoning_tokens": 0  // ä»…æ¨ç†æ¨¡å‹
    },
    "total_tokens": 29
  }
}
```

## ğŸš¨ é”™è¯¯å¤„ç†

### æ ‡å‡†é”™è¯¯æ ¼å¼

```json
{
  "error": {
    "type": "invalid_request_error",
    "code": "model_not_supported",
    "message": "The model 'gpt-3.5-turbo' is not supported for the responses endpoint.",
    "param": "model"
  }
}
```

### å¸¸è§é”™è¯¯

| é”™è¯¯ç  | è¯´æ˜ | è§£å†³æ–¹æ¡ˆ |
|--------|------|----------|
| `model_not_supported` | æ¨¡å‹ä¸æ”¯æŒ Responses API | ä½¿ç”¨æ”¯æŒçš„æ–°æ¨¡å‹ |
| `invalid_previous_response_id` | æ— æ•ˆçš„ä¸Šä¸€ä¸ªå“åº”ID | æ£€æŸ¥å“åº”IDæ˜¯å¦æ­£ç¡® |
| `tool_not_available` | å·¥å…·ä¸å¯ç”¨ | æ£€æŸ¥å·¥å…·é…ç½® |
| `max_tokens_exceeded` | è¶…å‡ºä»¤ç‰Œé™åˆ¶ | å‡å°‘è¾“å…¥æˆ–è®¾ç½®max_output_tokens |

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. çŠ¶æ€ç®¡ç†ç­–ç•¥

```python
class ConversationManager:
    def __init__(self, model="gpt-4.1", instructions="You are a helpful assistant."):
        self.model = model
        self.instructions = instructions
        self.last_response_id = None
    
    def send_message(self, input_text, tools=None):
        response = client.responses.create(
            model=self.model,
            input=input_text,
            instructions=self.instructions,
            previous_response_id=self.last_response_id,
            tools=tools or []
        )
        
        self.last_response_id = response.id
        return response.output[0].content[0].text
    
    def reset_conversation(self):
        self.last_response_id = None

# ä½¿ç”¨ç¤ºä¾‹
conv = ConversationManager()
print(conv.send_message("Hello, I'm Alice"))
print(conv.send_message("What's my name?"))  # ä¼šè®°ä½æ˜¯ Alice
```

### 2. å·¥å…·è°ƒç”¨ä¼˜åŒ–

```python
def smart_tool_calling(user_input):
    # æ ¹æ®è¾“å…¥æ™ºèƒ½é€‰æ‹©å·¥å…·
    available_tools = []
    
    if "weather" in user_input.lower():
        available_tools.append(weather_tool)
    if "calculate" in user_input.lower():
        available_tools.append(calculator_tool)
    if "search" in user_input.lower():
        available_tools.append(search_tool)
    
    response = client.responses.create(
        model="gpt-4.1",
        input=user_input,
        instructions="Use the appropriate tools to help the user.",
        tools=available_tools,
        tool_choice="auto"
    )
    
    return response
```

### 3. æ¨ç†æ¨¡å‹ä¼˜åŒ–

```python
def optimized_reasoning(complex_problem):
    response = client.responses.create(
        model="o3",
        input=complex_problem,
        instructions="Think step by step and show your reasoning process.",
        reasoning={
            "effort": "high"  # å¯¹å¤æ‚é—®é¢˜ä½¿ç”¨é«˜æ¨ç†åŠªåŠ›
        },
        temperature=0.1  # é™ä½éšæœºæ€§ä»¥è·å¾—ä¸€è‡´ç»“æœ
    )
    
    # åˆ†ææ¨ç†ä½¿ç”¨æƒ…å†µ
    reasoning_tokens = response.usage.output_tokens_details.reasoning_tokens
    total_cost = calculate_cost(response.usage)
    
    return {
        "answer": response.output[0].content[0].text,
        "reasoning_tokens": reasoning_tokens,
        "cost": total_cost
    }
```

## ğŸ”® æœªæ¥å‘å±•

### å³å°†æ¨å‡ºçš„åŠŸèƒ½

1. **å®Œæ•´çš„ Assistants API åŠŸèƒ½é›†æˆ**ï¼ˆ2026å¹´ä¸ŠåŠå¹´ï¼‰
2. **æ›´å¤šå†…ç½®å·¥å…·**ï¼šWebæœç´¢ã€è®¡ç®—æœºä½¿ç”¨ç­‰
3. **æ¨¡å‹ä¸Šä¸‹æ–‡åè®® (MCP) æ”¯æŒ**
4. **å¢å¼ºçš„å¤šæ¨¡æ€èƒ½åŠ›**

### è¿ç§»æ—¶é—´çº¿

- **ç°åœ¨**ï¼šå¯ä»¥å¼€å§‹ä½¿ç”¨ Responses API
- **2026å¹´ä¸ŠåŠå¹´**ï¼šåŠŸèƒ½å¯¹ç­‰ Assistants API
- **2026å¹´**ï¼šAssistants API å¼ƒç”¨å…¬å‘Š
- **2027å¹´**ï¼šå®Œå…¨è¿ç§»åˆ° Responses API

<Info>
  **å¼€å‘å»ºè®®**ï¼šæ–°é¡¹ç›®æ¨èç›´æ¥ä½¿ç”¨ Responses APIï¼Œç°æœ‰é¡¹ç›®å¯ä»¥é€æ­¥è¿ç§»ã€‚APIæ˜“ å°†æŒç»­è·Ÿè¿› OpenAI çš„æ›´æ–°ï¼Œç¡®ä¿åŠŸèƒ½å®Œæ•´æ€§ã€‚
</Info>

---

éœ€è¦æ›´å¤šå¸®åŠ©ï¼Ÿè¯·è®¿é—® [APIæ˜“å®˜ç½‘](https://api.apiyi.com) æˆ–æŸ¥çœ‹ [OpenAI Responses API å®˜æ–¹æ–‡æ¡£](https://platform.openai.com/docs/api-reference/responses)ã€‚